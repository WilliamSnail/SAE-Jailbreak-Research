{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e76b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Device: cuda\n",
      "PyTorch Version: 2.10.0+cu130\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from textwrap import dedent\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display, HTML\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Disable gradients for inference\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "# Reduce logging noise\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "# Device selection\n",
    "if t.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Torch Device: {device}\")\n",
    "print(f\"PyTorch Version: {t.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b66eff20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Target model:  google/gemma-3-4b-it\n",
      "  Attacker:      gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# CONFIGURATION — adjust these parameters for your experiments\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Target model (local, runs through NNSight)\n",
    "TARGET_MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "# Attacker / Judge model (remote, via OpenAI API)\n",
    "ATTACKER_MODEL = \"gpt-4o\"\n",
    "\n",
    "# NNSIGHT (False if using a local model)\n",
    "REMOTE = False\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Target model:  {TARGET_MODEL_NAME}\")\n",
    "print(f\"  Attacker:      {ATTACKER_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b81d2744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading target model: google/gemma-3-4b-it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Architecture: ['Gemma3ForConditionalGeneration']\n",
      "  Attention:    sdpa (PyTorch native)\n",
      "  Loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "print(f\"Loading target model: {TARGET_MODEL_NAME}\")\n",
    "model = LanguageModel(\n",
    "    TARGET_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=t.bfloat16,\n",
    "    # Use PyTorch's Scaled Dot-Product Attention (SDPA) for faster inference.\n",
    "    # Mathematically identical to default eager attention — same outputs, same hidden states,\n",
    "    # same SAE activations — just uses fused CUDA kernels for ~20-40% speedup.\n",
    "    # Built into PyTorch 2.0+, no extra install needed (unlike flash-attn which requires nvcc).\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "# Set padding for batched generation\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"  Architecture: {model.config.architectures}\")\n",
    "print(f\"  Attention:    sdpa (PyTorch native)\")\n",
    "print(f\"  Loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a62c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b4086cd9d343569a7a131a61ce1360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Speed test (3 prompts, max_new_tokens=200)\n",
      "============================================================\n",
      "  NNSight:        25.2 tok/s  (313 tokens in 12.43s)\n",
      "  HuggingFace:    27.2 tok/s  (308 tokens in 11.33s)\n",
      "============================================================\n",
      "  NNSight overhead: +9.7% vs vanilla HuggingFace\n",
      "  (Expected: NNSight adds tracing layer for interpretability access)\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Speed test — NNSight vs vanilla HuggingFace generation throughput\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "test_prompts = [\n",
    "    [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"Explain photosynthesis in simple terms.\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"Write a short poem about the ocean.\"}],\n",
    "]\n",
    "max_new = 200\n",
    "\n",
    "def bench_nnsight(prompts, max_new_tokens):\n",
    "    \"\"\"Benchmark NNSight-wrapped generation (what the pipeline uses).\"\"\"\n",
    "    total_tokens = 0\n",
    "    total_time = 0.0\n",
    "    for msgs in prompts:\n",
    "        prompt_text = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        input_len = len(tokenizer.encode(prompt_text))\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        with model.generate(prompt_text, max_new_tokens=max_new_tokens, repetition_penalty=1.2, remote=REMOTE) as gen:\n",
    "            tokens = model.generator.output.save()\n",
    "        elapsed = time.perf_counter() - t0\n",
    "\n",
    "        new_tokens = tokens.shape[-1] - input_len\n",
    "        total_tokens += new_tokens\n",
    "        total_time += elapsed\n",
    "    return total_tokens, total_time\n",
    "\n",
    "def bench_huggingface(prompts, max_new_tokens):\n",
    "    \"\"\"Benchmark vanilla HuggingFace model.generate() (no NNSight wrapper).\"\"\"\n",
    "    # Access the underlying HF model directly\n",
    "    hf_model = model._model\n",
    "    total_tokens = 0\n",
    "    total_time = 0.0\n",
    "    for msgs in prompts:\n",
    "        prompt_text = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "        input_len = input_ids.shape[-1]\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        with t.no_grad():\n",
    "            output_ids = hf_model.generate(input_ids, max_new_tokens=max_new_tokens, repetition_penalty=1.2)\n",
    "        elapsed = time.perf_counter() - t0\n",
    "\n",
    "        new_tokens = output_ids.shape[-1] - input_len\n",
    "        total_tokens += new_tokens\n",
    "        total_time += elapsed\n",
    "    return total_tokens, total_time\n",
    "\n",
    "# Warmup run (first call is always slower due to CUDA graph compilation)\n",
    "print(\"Warming up...\")\n",
    "_ = bench_nnsight(test_prompts[:1], 10)\n",
    "_ = bench_huggingface(test_prompts[:1], 10)\n",
    "\n",
    "# Benchmark NNSight\n",
    "print(f\"\\nSpeed test ({len(test_prompts)} prompts, max_new_tokens={max_new})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "nn_tokens, nn_time = bench_nnsight(test_prompts, max_new)\n",
    "nn_tps = nn_tokens / nn_time if nn_time > 0 else 0\n",
    "print(f\"  NNSight:      {nn_tps:6.1f} tok/s  ({nn_tokens} tokens in {nn_time:.2f}s)\")\n",
    "\n",
    "hf_tokens, hf_time = bench_huggingface(test_prompts, max_new)\n",
    "hf_tps = hf_tokens / hf_time if hf_time > 0 else 0\n",
    "print(f\"  HuggingFace:  {hf_tps:6.1f} tok/s  ({hf_tokens} tokens in {hf_time:.2f}s)\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "overhead_pct = 100 * (nn_time - hf_time) / hf_time if hf_time > 0 else 0\n",
    "print(f\"  NNSight overhead: {overhead_pct:+.1f}% vs vanilla HuggingFace\")\n",
    "print(f\"  (Expected: NNSight adds tracing layer for interpretability access)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
